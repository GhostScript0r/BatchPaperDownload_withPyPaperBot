{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "Need to run `pip install PyPaperBot` first, if the Python module is not yet installed.\n",
    "\n",
    "See https://github.com/ferru97/PyPaperBot for more information about the module PythonPaperBot.\n",
    "\n",
    "This script works in Windows. To run in Linux, change the line `process=subprocess.Popen([\"powershell.exe\",\"-Command\",command],stdout=subprocess.PIPE)` and switch out powershell.exe for the Linux executables.\n",
    "\n",
    "#### Instruction\n",
    "\n",
    "- Create an empty folder and put this Jupyter Notebook in it. Make sure the folder path is not too long (if working on a network drive with very long path: mount the path as net drive first to shorten the path).\n",
    "\n",
    "- Put the keywords you want to search for in Keywords\\_\\*.txt files. You can create multiple Keywords\\_\\*.txt files. The python script will generate all possible combinations of keywords and store them in a text file \"Search_Terms.txt\"\n",
    "\n",
    "- Change the variable `download_number_per_keyword` and `year_from` according to your preference.\n",
    "- Run the \"Download papers\" section. The script will create a new folder for each search terms and store the downloaded paper inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a tor session\n",
    "This part does not work as tor is apparently blocked by my ISP. If you can get a tor proxy, you can add proxy settings in the PyPyperBot argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_tor_session():\n",
    "    from tor_proxy import tor_proxy\n",
    "    port=tor_proxy()\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:'+port,\n",
    "                       'https': 'socks5://127.0.0.1:'+port}\n",
    "    return session\n",
    "session = get_tor_session()\n",
    "print(session.get(\"http://httpbin.org/ip\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import glob\n",
    "import pandas\n",
    "download_number_per_keyword=80 # Try to download the first so many search results. Each page of Google Scholar contains 10 search results, so 80 will mean downloading all papers shown in the first 8 pages.\n",
    "year_from=1956 # Limitation of how old the paper can be\n",
    "KeywordsListFiles=glob.glob(\"Keywords_*.txt\")\n",
    "Keywords=[[]]*len(KeywordsListFiles)\n",
    "for i in range(len(KeywordsListFiles)):\n",
    "    with open(KeywordsListFiles[i],'r',encoding='utf-8') as f:\n",
    "        keyw=f.read().splitlines()\n",
    "        Keywords[i]=keyw\n",
    "    i+=1\n",
    "Keywords_Combinations=list(itertools.product(*Keywords))\n",
    "Keywords_Combinations.reverse() # Make sure the latest added keywords are searched first\n",
    "f=open(\"Search_Terms.txt\",'w')\n",
    "f.close()\n",
    "import subprocess\n",
    "import sys\n",
    "# ________Get a tor proxy_________\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "# last_time_proxy_change = datetime.now() - relativedelta(years=1) # Put the first time stamp one year ago\n",
    "for keywordcombi in Keywords_Combinations:\n",
    "    # if datetime.now() - last_time_proxy_change > timedelta(hours=8):\n",
    "    #     tor_port=tor_proxy() # Get a new proxy \n",
    "    #     last_time_proxy_change=datetime.now()\n",
    "    combi=\" \".join(keywordcombi)\n",
    "    with open(\"Search_Terms.txt\",'a+',encoding='utf-8') as f:\n",
    "        f.write(combi+\"\\n\")\n",
    "    already_downloaded=False\n",
    "    try:\n",
    "        os.mkdir(combi)\n",
    "    except FileExistsError:\n",
    "        try:\n",
    "            Downloaded_Papers=pandas.read_csv(combi+\"\\\\result.csv\",header=0)\n",
    "            if Downloaded_Papers[\"Downloaded\"].sum() > download_number_per_keyword * 0.5:\n",
    "                already_downloaded=True\n",
    "        except FileNotFoundError:\n",
    "            print(\"Folder \\\"\"+combi+\"\\\" is created but empty\")\n",
    "    # if already_downloaded:\n",
    "    #     print(\"There's already some documents downloaded for the title: \"+combi)\n",
    "    if not already_downloaded:\n",
    "        command=\"python.exe -m PyPaperBot --query=\\\"\"+combi+\"\\\" --scholar-pages=\"+str(round(download_number_per_keyword/10))+\"  --min-year=\"+str(year_from)+\" --dwn-dir=\\\".\\\\\"+combi+\"\\\"\"\n",
    "        # command=\"for($i=1;$i -lt 3; $i++) {Write-Host \\\"$($i) \"+combi+\"\\\"; Start-Sleep -s 1}\" # This is a test command to see if the following subprocess call works.\n",
    "        print(\"Searching for \\\"\"+combi+\"\\\" and downloading...\")\n",
    "        process=subprocess.Popen([\"powershell.exe\",\"-Command\",command],stdout=subprocess.PIPE)\n",
    "        for c in iter(lambda: process.stdout.read(1), b''):\n",
    "            sys.stdout.write(c.decode('utf-8','ignore'))\n",
    "import glob\n",
    "for f in glob.glob('./**/(2)*.pdf',recursive=True):\n",
    "    os.remove(f) # remove files with (2) in the beginning\n",
    "    print(\"\\\"\",f,\"\\\"removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check repeated files and remove duplicated files\n",
    "\n",
    "This part will create a file called Duplication.json\n",
    "\n",
    "If the \"Download paper\" session was interrupted while downloading and then run again, the already downloaded paper will be downloaded again with \"(2)\" added to the beginning of file name. The following script assumes that all files with \"(2)\" in its name is a duplicate and will delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "for f in glob.glob('./**/(2)*.pdf',recursive=True):\n",
    "    os.remove(f) # remove files with (2) in the beginning\n",
    "    print(\"\\\"\",f,\"\\\"removed.\")\n",
    "import pandas\n",
    "import json\n",
    "Downloaded_PDFs={}\n",
    "for (dirpath, dirnames, filenames) in os.walk(os.getcwd()):\n",
    "    if \".pdf\" in str(filenames):\n",
    "        for Idx in [\"bibtex.bib\",\"result.csv\",\"Thumbs.db\"]:\n",
    "            try:\n",
    "                filenames.remove(Idx) # Remove bibtex.bib, result.csv and possibly Thumbs.db from the scanned list, leaving only PDF files.\n",
    "            except:\n",
    "                pass\n",
    "        Downloaded_PDFs[os.path.basename(dirpath)]=filenames # This is a dictionary\n",
    "Downloaded_Keywords=Downloaded_PDFs.keys() # this is essentially the same as the variable \"combi\" above, but may have different sorting. So here another variable. \n",
    "Downloaded_PDFs_List=Downloaded_PDFs.values()\n",
    "Downloaded_PDFs_List_Flat=[\n",
    "    x\n",
    "    for xs in Downloaded_PDFs_List\n",
    "    for x in xs\n",
    "]\n",
    "Downloaded_PDF_List_Full_NoDuplicate=list(dict.fromkeys(Downloaded_PDFs_List_Flat))\n",
    "PDF_Repitition_Count=[1]*len(Downloaded_PDFs_List_Flat)\n",
    "PDF_File_Already_Appeared=[False]*len(Downloaded_PDFs_List_Flat)\n",
    "PDF_Repitition_List={}\n",
    "for i in range(len(Downloaded_PDFs_List_Flat)):\n",
    "    PDF_Repitition_Count[i]=Downloaded_PDFs_List_Flat.count(Downloaded_PDFs_List_Flat[i])\n",
    "    if PDF_Repitition_Count[i] > 1:\n",
    "        if Downloaded_PDFs_List_Flat[i] in Downloaded_PDFs_List_Flat[:i]:\n",
    "            PDF_File_Already_Appeared[i]=True\n",
    "        if not PDF_File_Already_Appeared[i]:\n",
    "            PDF_Appear_In=[]\n",
    "            for key in Downloaded_PDFs:\n",
    "                if Downloaded_PDFs_List_Flat[i] in Downloaded_PDFs[key]:\n",
    "                    PDF_Appear_In.append(key)\n",
    "            PDF_Repitition_List[Downloaded_PDFs_List_Flat[i]]=PDF_Appear_In\n",
    "print(str(len(Downloaded_PDFs_List_Flat)),\"PDFs downloaded. Unique PDF files:\",str(len(Downloaded_PDF_List_Full_NoDuplicate)))\n",
    "# PDF_Repitition_Table=pandas.DataFrame(dict([(key, pandas.Series(value)) for key, value in PDF_Repitition_List.items()]))\n",
    "import json\n",
    "with open(\"Duplication.json\",\"w\",encoding=\"utf-8\") as fp:\n",
    "    json.dump(PDF_Repitition_List,fp)\n",
    "with open(\"Duplication.json\",'r',encoding=\"utf-8\") as f:\n",
    "    PDF_Repitition_List=json.load(f)\n",
    "for key in PDF_Repitition_List:\n",
    "    for i in range(len(PDF_Repitition_List[key])):\n",
    "        if i !=0:\n",
    "            Folder_Name=PDF_Repitition_List[key][i]\n",
    "            File_Name=key.split(\".pdf\")[0]+\".pdf\"\n",
    "            try:\n",
    "                os.remove(Folder_Name+'\\\\'+File_Name)\n",
    "                print('Duplicated PDF file \"'+Folder_Name+'\\\\'+File_Name+'\" removed.')\n",
    "            except FileNotFoundError:\n",
    "                print('Duplicated PDF file \"'+Folder_Name+'\\\\'+File_Name+'\" was already removed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List papers by DOI\n",
    "\n",
    "This part will check the `result.csv` file from each subfolder and make a list (pandas DataFrame) of all downloaded papers.\n",
    "\n",
    "If paper download of one specific search term is interrupted, the download will be incomplete and no `result.csv` or `bibtex.bib` file will be created. The folder will be skipped.\n",
    "\n",
    "The list (DataFrame) will be stored in `List_Papers.json` and will look like this:\n",
    "```\n",
    "{\n",
    "        \"Name\": \"[Title of the paper]\",\n",
    "        \"DOI\": \"[DOI of the paper]\",\n",
    "        \"Year\": [Year in int],\n",
    "        \"Journal\": \"[Journal Name]\",\n",
    "        \"Search terms\": [\n",
    "            \"List\",\n",
    "            \"of\",\n",
    "            \"Search\",\n",
    "            \"Terms\",\n",
    "            \"where\",\n",
    "            \"this\",\n",
    "            \"paper\",\n",
    "            \"appeared\"\n",
    "        ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas\n",
    "Columns_in_CSV=['Name','DOI','Year','Journal','PDF Name'] #'Scholar Link','Authors'\n",
    "List_Papers=pandas.DataFrame(columns=Columns_in_CSV+['Search term']) #{\"doi\":[], \"title\":[], \"searchterms\":[] }\n",
    "for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "    for filename in [f for f in filenames if f==\"result.csv\"]: # f.endswith(\".csv\")]:\n",
    "        search_term=dirpath.replace(\".\\\\\",\"\")\n",
    "        search_results=pandas.read_csv(os.path.join(dirpath, filename),header=0)\n",
    "        search_results.drop(columns=search_results.columns.difference(Columns_in_CSV+['Downloaded']),inplace=True) \n",
    "        search_results=search_results.loc[search_results.Downloaded,:] # Remove not downloaded entries (where \"Downloaded=False\")\n",
    "        search_results.drop(columns=['Downloaded'],inplace=True) \n",
    "        search_results[\"Search term\"]=[search_term]*len(search_results)\n",
    "        List_Papers=pandas.concat([List_Papers,search_results])\n",
    "List_Papers.fillna(\"Unknown\",inplace=True)\n",
    "List_Papers[\"Duplicated\"]=List_Papers[\"PDF Name\"].duplicated()\n",
    "List_Papers[\"Duplicated_Count\"]=[0]*len(List_Papers.index) # Create a new column in DataFrame with zeros. len(List_Papers.index) can also be List_Papers.shape[0] (shape returns a 2D-tuple with num of rows and num of columns)\n",
    "List_Papers[\"Duplicated_First_Occurance\"]=[-1]*len(List_Papers.index)\n",
    "Downloaded_Papers=[]\n",
    "# for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "#     for filename in [f for f in filenames if f.endswith(\".pdf\")]:\n",
    "#         Downloaded_Papers=Downloaded_Papers+[filename]\n",
    "# Papers_NoDuplicate=List_Papers[List_Papers[\"PDF Name\"].isin(Downloaded_Papers)]\n",
    "for i in range(1,len(List_Papers.index)):\n",
    "    if list(List_Papers[\"Duplicated\"])[i]:\n",
    "        Current_Paper_Name=List_Papers.iloc[i,List_Papers.columns.get_loc(\"PDF Name\")]\n",
    "        Previous_Papers=list(List_Papers[\"PDF Name\"])[slice(i)]\n",
    "        List_Papers.iloc[i,List_Papers.columns.get_loc(\"Duplicated_Count\")]=Previous_Papers.count(Current_Paper_Name)\n",
    "        Current_Paper_First_Occurance=Previous_Papers.index(Current_Paper_Name)\n",
    "        List_Papers.iloc[i,List_Papers.columns.get_loc(\"Duplicated_First_Occurance\")]=Current_Paper_First_Occurance\n",
    "        Search_Terms=List_Papers.iloc[Current_Paper_First_Occurance,List_Papers.columns.get_loc(\"Search term\")]+\",\"+List_Papers.iloc[i,List_Papers.columns.get_loc(\"Search term\")]\n",
    "        List_Papers.iloc[Current_Paper_First_Occurance,List_Papers.columns.get_loc(\"Search term\")]=Search_Terms\n",
    "List_Papers=List_Papers.loc[~List_Papers[\"Duplicated\"],:]\n",
    "Search_Terms_Lists=[ [] for _ in range(len(List_Papers.index)) ]\n",
    "List_Papers.drop(list(List_Papers.filter(regex='Duplicated|PDF Name')), axis=1, inplace=True)\n",
    "for i in range(len(List_Papers.index)):\n",
    "    Search_Terms_List=List_Papers.iloc[i,List_Papers.columns.get_loc(\"Search term\")].split(\",\")\n",
    "    Search_Terms_Lists[i]=Search_Terms_List\n",
    "List_Papers[\"Search terms\"]=Search_Terms_Lists\n",
    "List_Papers.drop(columns=['Search term'],inplace=True)\n",
    "List_Papers.to_json(\"List_Papers.json\",orient=\"records\")\n",
    "import json\n",
    "with open(\"List_Papers.json\",'r',encoding=\"utf-8\") as fp:\n",
    "    JSON_Data=json.load(fp)\n",
    "    JSON_Data_Formatted=json.dumps(JSON_Data,indent=4)\n",
    "with open(\"List_Papers.json\",'w',encoding=\"utf-8\") as fp:\n",
    "    fp.write(JSON_Data_Formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
